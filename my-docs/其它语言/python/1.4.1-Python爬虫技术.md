## Python 爬虫技术
> Create by yelv on 21 March Recently revised in 21 March 2025
### BeautifulSoup4 使用

1. 安装和导入
```python
pip install beautifulsoup4
from bs4 import BeautifulSoup
```

2. 基本用法
```python
# 创建 BeautifulSoup 对象
soup = BeautifulSoup(html_doc, 'html.parser')

# 查找元素
soup.find('div')  # 查找第一个 div 标签
soup.find_all('p')  # 查找所有 p 标签
soup.select('.class_name')  # 使用 CSS 选择器
soup.select('#id_name')

# 获取内容
element.text  # 获取文本内容
element['href']  # 获取属性值
```

### urllib3 网络请求

1. 基本使用
```python
import urllib3

# 创建连接池管理器
http = urllib3.PoolManager()

# GET 请求
response = http.request('GET', 'http://example.com')

# POST 请求
response = http.request(
    'POST',
    'http://example.com/post',
    fields={'key': 'value'}
)

# 添加请求头
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
}
response = http.request('GET', 'http://example.com', headers=headers)
```

### 反爬虫策略应对

1. 请求头伪装
```python
headers = {
    'User-Agent': '常见浏览器的 UA',
    'Referer': '合理的来源页面',
    'Cookie': '必要的 Cookie 信息'
}
```

2. 请求频率控制
```python
import time

def crawl_with_delay(urls, delay=2):
    for url in urls:
        # 爬取操作
        time.sleep(delay)  # 添加延时
```

3. IP 代理池
```python
proxies = {
    'http': 'http://10.10.10.1:1234',
    'https': 'https://10.10.10.1:1234'
}
response = http.request('GET', 'http://example.com', proxies=proxies)
```